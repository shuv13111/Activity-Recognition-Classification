{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea4896db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.ml.feature import VectorAssembler,StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "628512d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"AReM\").getOrCreate()\n",
    "\n",
    "# Read the feature data (X) from CSV\n",
    "data = spark.read.csv(\"data.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47d00b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------+---------+---------+---------+-------+\n",
      "|avg_rss12|var_rss12|avg_rss13|var_rss13|avg_rss23|var_rss23|  label|\n",
      "+---------+---------+---------+---------+---------+---------+-------+\n",
      "|    36.75|     4.44|    13.33|     2.62|     12.5|      1.5|walking|\n",
      "|     33.0|     4.95|     16.5|     4.56|    13.75|     2.59|walking|\n",
      "|    31.75|     5.07|    16.75|     2.86|     17.5|     2.29|walking|\n",
      "|    36.75|     4.92|    15.75|     2.28|      9.5|      4.5|walking|\n",
      "|     34.5|     5.02|    14.75|     1.09|     18.5|     3.57|walking|\n",
      "|     26.0|     3.24|     17.0|     3.08|    17.75|     1.79|walking|\n",
      "|    40.75|      3.7|      9.5|     4.97|     12.0|     2.83|walking|\n",
      "|    36.25|     0.43|    14.25|      3.7|    18.67|     2.62|walking|\n",
      "|     35.0|     2.92|    13.67|     3.09|     11.0|      0.0|walking|\n",
      "|    33.75|     8.26|    12.25|     1.92|    12.25|     6.94|walking|\n",
      "|     31.0|     1.41|     12.0|      5.1|     19.0|     0.82|walking|\n",
      "|     26.0|     3.67|     17.0|     3.24|    14.25|     1.92|walking|\n",
      "|     41.0|     4.06|     16.5|     3.84|    11.67|     5.31|walking|\n",
      "|     37.5|      0.5|     16.5|      4.5|    11.25|      3.9|walking|\n",
      "|     37.8|     3.06|     15.6|      3.2|    16.25|     2.95|walking|\n",
      "|    42.33|     2.49|    14.67|     0.47|     15.0|     4.12|walking|\n",
      "|     33.5|     3.84|     5.75|     2.49|     22.0|     2.12|walking|\n",
      "|     25.5|      5.5|    14.67|      9.1|    18.75|     2.86|walking|\n",
      "|     33.0|     7.18|    17.75|     4.66|    17.25|     7.08|walking|\n",
      "|    36.75|      1.3|    21.67|      3.4|    20.33|     1.25|walking|\n",
      "+---------+---------+---------+---------+---------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac082f9",
   "metadata": {},
   "source": [
    "## There are 2 classes, bending1 and bending2 which are similar, lets call them bending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d993815c-14bc-4645-9556-364c0d1b80f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"label\", when((data[\"label\"] == \"bending1\") | (data[\"label\"] == \"bending2\"), \"bending\").otherwise(data[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "82f1381b-c01f-43ba-8bb3-b213ba5d50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9af2625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|   label|count|\n",
      "+--------+-----+\n",
      "| walking| 7200|\n",
      "| cycling| 7198|\n",
      "| bending| 5760|\n",
      "|standing| 7200|\n",
      "| sitting| 7199|\n",
      "|   lying| 7200|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902ddfd-8990-4b9b-8317-997210293d18",
   "metadata": {},
   "source": [
    "### We have 6 classes now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576052e",
   "metadata": {},
   "source": [
    "## In activity recognition applications, correctly identifying both positive and negative instances is important. However, depending on the problem, the cost of false positives and false negatives may not be equal. For example, in medical diagnosis, a false negative (not detecting a disease when it's present) may be more costly than a false positive (detecting a disease when it's not present).\n",
    "\n",
    "## In the case of activity recognition, it's important to minimize false negatives (i.e., failing to detect an activity that's actually being performed) in order to ensure accurate tracking of physical activity levels. Failing to detect certain activities could lead to inaccurate assessment of the user's overall physical activity, which could have negative impacts on healthcare decision-making and outcomes.\n",
    "\n",
    "## Justification for Selecting F1 Score:\n",
    "## The F1 score is the harmonic mean of precision and recall, which makes it a suitable choice for binary classification tasks, especially when dealing with imbalanced datasets. In the case of the Random Forest model, the F1 score is slightly higher than the other metrics, indicating a balanced trade-off between correctly identifying positive cases (precision) and capturing all positive cases (recall). This balance is important when the cost of false positives and false negatives is not significantly skewed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e34c1-d262-4868-bed9-91ef6da48fec",
   "metadata": {},
   "source": [
    "### **As per the project requirement, we will be running Decision Tree, Random Forest and Multilayer Perceptron and eventually evaluate the results**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677d8356-98d8-41cf-bdd0-414fde8fef95",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7843c1d1-777d-40ce-92f1-e186108dae56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+------------------+------------------+------------------+\n",
      "|   ModelName|          Accuracy|                F1|            Recall|         Precision|\n",
      "+------------+------------------+------------------+------------------+------------------+\n",
      "|DecisionTree|0.6414507772020726|0.6375003350188141|0.6414507772020726|0.6485930520980879|\n",
      "+------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define feature columns (including the \"label\" column)\n",
    "feature_columns = data.columns\n",
    "\n",
    "# Exclude the \"label\" column\n",
    "feature_columns.remove(\"label\")\n",
    "\n",
    "# Create a VectorAssembler\n",
    "vector_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Create a StringIndexer for label indexing\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"indexed_label\")\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "decision_tree = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"indexed_label\")\n",
    "\n",
    "# Create a Pipeline\n",
    "pipeline = Pipeline(stages=[vector_assembler, label_indexer, decision_tree])\n",
    "\n",
    "# Fit the pipeline on the training data (assuming data is already split)\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Initialize the MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"indexed_label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Calculate accuracy, F1 score, recall, and precision\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Create a new DataFrame with model name and evaluation metrics\n",
    "results = spark.createDataFrame([Row(ModelName=\"DecisionTree\", Accuracy=accuracy, F1=f1, Recall=recall, Precision=precision)])\n",
    "\n",
    "# Show the results\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c5ba06-355c-4b93-b6ce-f2022cf134a7",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "420d6966-bc0a-47e8-a3e5-e7b6112a5f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+------------------+------------------+------------------+\n",
      "|   ModelName|          Accuracy|                F1|            Recall|         Precision|\n",
      "+------------+------------------+------------------+------------------+------------------+\n",
      "|DecisionTree|0.6414507772020726|0.6375003350188141|0.6414507772020726|0.6485930520980879|\n",
      "|RandomForest|0.6761259465922679|0.6716172617717742|0.6761259465922679| 0.671193639541631|\n",
      "+------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Create a RandomForest classifier\n",
    "random_forest = RandomForestClassifier(featuresCol=\"features\", labelCol=\"indexed_label\")\n",
    "\n",
    "# Create a Pipeline for the RandomForest model\n",
    "rf_pipeline = Pipeline(stages=[vector_assembler, label_indexer, random_forest])\n",
    "\n",
    "# Fit the RandomForest model on the training data\n",
    "rf_model = rf_pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions with the RandomForest model on the test data\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "\n",
    "# Initialize the MulticlassClassificationEvaluator for the RandomForest model\n",
    "rf_evaluator = MulticlassClassificationEvaluator(labelCol=\"indexed_label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Calculate accuracy, F1 score, recall, and precision for the RandomForest model\n",
    "rf_accuracy = rf_evaluator.evaluate(rf_predictions, {rf_evaluator.metricName: \"accuracy\"})\n",
    "rf_f1 = rf_evaluator.evaluate(rf_predictions, {rf_evaluator.metricName: \"f1\"})\n",
    "rf_recall = rf_evaluator.evaluate(rf_predictions, {rf_evaluator.metricName: \"weightedRecall\"})\n",
    "rf_precision = rf_evaluator.evaluate(rf_predictions, {rf_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Create a new DataFrame with RandomForest model results\n",
    "rf_results = spark.createDataFrame([Row(ModelName=\"RandomForest\", Accuracy=rf_accuracy, F1=rf_f1, Recall=rf_recall, Precision=rf_precision)])\n",
    "\n",
    "# Union the DataFrames to combine results\n",
    "all_results = results.union(rf_results)\n",
    "\n",
    "# Show the combined results\n",
    "all_results.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce06f2f-505c-4450-984c-a4214782b18c",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fce5f2c1-12e0-4eb2-8b54-b32ce341347c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+------------------+------------------+------------------+\n",
      "|   ModelName|          Accuracy|                F1|            Recall|         Precision|\n",
      "+------------+------------------+------------------+------------------+------------------+\n",
      "|DecisionTree|0.6414507772020726|0.6375003350188141|0.6414507772020726|0.6485930520980879|\n",
      "|RandomForest|0.6761259465922679|0.6716172617717742|0.6761259465922679| 0.671193639541631|\n",
      "|         MLP|0.6076524511757673|0.5797847862621367|0.6076524511757673|0.6060020664164829|\n",
      "+------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "# Create a Multilayer Perceptron classifier\n",
    "layers = [len(feature_columns), 10, 5, len(data.select(\"label\").distinct().collect())]\n",
    "mlp = MultilayerPerceptronClassifier(featuresCol=\"features\", labelCol=\"indexed_label\", layers=layers, seed=1234)\n",
    "\n",
    "# Create a Pipeline for the MLP model\n",
    "mlp_pipeline = Pipeline(stages=[vector_assembler, label_indexer, mlp])\n",
    "\n",
    "# Fit the MLP model on the training data\n",
    "mlp_model = mlp_pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions with the MLP model on the test data\n",
    "mlp_predictions = mlp_model.transform(test_data)\n",
    "\n",
    "# Initialize the MulticlassClassificationEvaluator for the MLP model\n",
    "mlp_evaluator = MulticlassClassificationEvaluator(labelCol=\"indexed_label\", predictionCol=\"prediction\")\n",
    "\n",
    "# Calculate accuracy, F1 score, recall, and precision for the MLP model\n",
    "mlp_accuracy = mlp_evaluator.evaluate(mlp_predictions, {mlp_evaluator.metricName: \"accuracy\"})\n",
    "mlp_f1 = mlp_evaluator.evaluate(mlp_predictions, {mlp_evaluator.metricName: \"f1\"})\n",
    "mlp_recall = mlp_evaluator.evaluate(mlp_predictions, {mlp_evaluator.metricName: \"weightedRecall\"})\n",
    "mlp_precision = mlp_evaluator.evaluate(mlp_predictions, {mlp_evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "# Create a new DataFrame with MLP model results\n",
    "mlp_results = spark.createDataFrame([Row(ModelName=\"MLP\", Accuracy=mlp_accuracy, F1=mlp_f1, Recall=mlp_recall, Precision=mlp_precision)])\n",
    "\n",
    "# Union the DataFrames to combine all results\n",
    "all_results = all_results.union(mlp_results)\n",
    "\n",
    "# Show the combined results\n",
    "all_results.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cdc0e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's analyze the performance of each model:\n",
    "\n",
    "**Decision Tree**:\n",
    "\n",
    "Accuracy: 64.14%\n",
    "F1 Score: 63.75%\n",
    "Recall: 64.14%\n",
    "Precision: 64.86%\n",
    "\n",
    "**Performance Analysis**: The Decision Tree model performs reasonably well with an accuracy of 64.14%. It has a good balance between precision and recall, as indicated by the F1 score of 63.75%. It is a suitable choice if you prefer a straightforward model with decent performance.\n",
    "\n",
    "**Random Forest**:\n",
    "\n",
    "Accuracy: 67.61%\n",
    "F1 Score: 67.16%\n",
    "Recall: 67.61%\n",
    "Precision: 67.12%\n",
    "\n",
    "**Performance Analysis**: The Random Forest model outperforms the Decision Tree with an accuracy of 67.61% and an F1 score of 67.16%. The F1 score is an excellent metric for this model because it balances precision and recall, making it suitable for binary classification tasks with imbalanced data. The Random Forest's ability to handle complex relationships in the data and reduce overfitting makes it a robust choice.\n",
    "\n",
    "**Multilayer Perceptron (MLP)**:\n",
    "\n",
    "Accuracy: 60.77%\n",
    "F1 Score: 57.98%\n",
    "Recall: 60.77%\n",
    "Precision: 60.60%\n",
    "\n",
    "**Performance Analysis**: The MLP model has the lowest accuracy and F1 score among the three models, indicating that it may not be the best choice for this particular dataset. It shows relatively lower performance in terms of precision and recall, suggesting that it might struggle to capture the underlying patterns in the data.\n",
    "\n",
    "\n",
    "Additionally, the Random Forest model often performs well on imbalanced datasets, as it can handle class imbalances and reduce overfitting. Therefore, the F1 score is a robust choice for evaluating and selecting the Random Forest model as the best model for this specific problem.\n",
    "\n",
    "In summary, the **Random Forest** model with an F1 score of 67.16% is selected as the best model for this classification task due to its balanced performance and ability to handle imbalanced data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
